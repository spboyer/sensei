# Project Context

- **Owner:** Shayne Boyer (spboyer@live.com)
- **Project:** GitHub Pages marketing site for Sensei â€” Astro + Tailwind CSS, Danish minimalist design, black/gray/orange palette
- **Stack:** Astro, Tailwind CSS, GitHub Pages, GitHub Actions
- **Created:** 2026-02-09

## Learnings

<!-- Append new learnings below. Each entry is something lasting about the project. -->

ðŸ“Œ Team update (2026-02-09): Tailwind v4 uses @tailwindcss/vite, not @astrojs/tailwind; CSS-first config via @theme directives â€” decided by Rusty
ðŸ“Œ Team update (2026-02-09): Astro site lives in docs/ subdirectory; worktree on feat/gh-pages-site branch â€” decided by Rusty
ðŸ“Œ Team update (2026-02-09): Landing page complete â€” 7 components (Hero, Problem, HowItWorks, BeforeAfter, ScoringLevels, QuickStart, Footer), all self-contained, no external JS â€” built by Linus
ðŸ“Œ Content update (2026-02-09): Social posts rewritten for platform compliance â€” LinkedIn: no markdown, links in first comment, engagement hooks at end, personal voice from Shayne. X: thread format for all posts, 280 char limit per tweet, 0 hashtags. All 4 post angles preserved (Launch, Before/After, Skill Collision, OSS Invite) â€” by Basher
ðŸ“Œ Content update (2026-02-09): Blog post (sensei-launch.md) reviewed and strengthened â€” added byline, scale paragraph in hook, contributor CTA in closing section. Core flow (Hook â†’ Checks â†’ Before/After â†’ Ralph Loop â†’ Getting Started â†’ CTA) validated as solid â€” by Basher
ðŸ“Œ Content rule (2026-02-09): LinkedIn posts must never contain markdown (backticks, code blocks, headers) â€” the platform doesn't render it and it looks broken. Use plain text, unicode, and emoji for formatting instead â€” established by Basher
ðŸ“Œ Content rule (2026-02-09): LinkedIn posts should use "Link in first comment" pattern â€” putting URLs in the post body kills algorithmic reach. Note with ðŸ”— emoji at end â€” established by Basher
ðŸ“Œ Content fix (2026-02-17): Blog post (sensei-launch.md) â€” re-applied two previously-approved fixes that got overwritten: (1) Broadened Anthropic reference from "builds on" to "informed by" â€” Sensei draws from multiple influences, not just Anthropic's spec. (2) Replaced Azure-specific INVOKES/FOR SINGLE OPERATIONS examples with generic pdf-tools MCP examples matching the post's pdf-processor theme. Key file: docs/blog/sensei-launch.md â€” by Basher
ðŸ“Œ Team update (2026-02-17): Blog post examples must use generic/themed references (pdf-processor), not Azure-specific MCP tools; Anthropic uses "informed by" framing â€” decided by Basher
ðŸ“Œ Content update (2026-02-17): Blog intro rewritten for harder impact â€” replaced passive opening ("You wrote a skill") with a visceral debugging scenario (agent calls wrong skill, 30-min debug, root cause is frontmatter). Subtitle swapped from mission statement to pain-point hook. Removed Azure reference from opening paragraph. Added urgency paragraph with 3â†’15â†’50 skill scaling and "combinatorial" framing pulled forward. Added "you'll never find it in your code" closer before Sensei reveal. "What Is Sensei?" section preserved but added "One command. No guessing." to the money line for stronger landing. Anthropic line on line 25 untouched per decisions.md. Key lesson: LinkedIn "see more" cutoff (~210 chars) must be a complete emotional beat, not a setup â€” by Basher
ðŸ“Œ Content rewrite (2026-02-17): Full blog post rewrite (sensei-launch.md) â€” restructured from feature-list format to persuasive Spec Kit playbook structure. Key structural changes: (1) Added "The Invisible Thing: Skill Collision" section â€” names the problem readers have been experiencing but couldn't articulate, giving them language for it. (2) Added "Why Now" section â€” urgency framing around ecosystem inflection point (skills everywhere, quality tooling nowhere). (3) Condensed 10 individual checks (10 sub-headers) into 3 category tables (Identity/Routing/Integration) â€” scannable, not exhausting. (4) Added "You've Been Here" with 3 concrete scenarios (document suite collision, MCP tool shadow, scale wall) â€” reader nods, not clinical examples. (5) Promoted Ralph Loop from buried numbered list to hero section with "Most Linters Find Problems. Sensei Fixes Them." headline â€” this is the differentiator and it was undersold. (6) Added "Why This Works" section â€” 3 principles (triggers+anti-triggers as pair, token budgets prevent bloat, iterative beats one-shot) explaining theory not just mechanics. (7) Rewrote CTA from "open for contributions" to "The Skill Ecosystem Needs Quality Tooling. Help Us Build It." â€” movement framing, not download framing. (8) Collapsed Getting Started from 3 sub-sections to install+run, removed token CLI details (belongs in repo docs, not blog). Before/After streamlined to single Lowâ†’High transformation instead of three separate blocks. All facts verified against README.md and SKILL.md. Anthropic line preserved. pdf-processor theme maintained throughout. Key lesson: blog posts sell by making the reader feel the problem before showing the solution â€” feature lists belong in docs, not articles â€” by Basher
ðŸ“Œ Research integration (2026-02-17): Wove 5 research findings into sensei-launch.md to add credibility and urgency. Changes: (1) "Skill Collision" section â€” added GitHub blog finding (2,500+ repos, "most agent files fail because they're too vague") and lost-in-the-middle positional bias research, connecting both to why explicit triggers/anti-triggers solve the problem. (2) "Why Now" section â€” added 26% vulnerability stat from arxiv agent skills research and maintenance drift framing (skills reference removed tools, nobody updates frontmatter). (3) "You've Been Here" â€” added Scenario 4 "The ghost instruction" about maintenance drift (deprecated tool still in INVOKES line, causes silent failures). (4) "Why This Works" â€” rewrote triggers/anti-triggers principle to explain LLM pattern-matching mechanics and connect back to lost-in-the-middle research. (5) CTA section â€” added Patronus AI multi-agent finding (unclear responsibilities â†’ misclassified tasks, cascading errors, hallucination). Key data points used: GitHub 2,500+ repos analysis, 26% vulnerability rate, lost-in-the-middle positional bias, Patronus AI multi-agent coordination failures. Total added: ~250 words, surgically integrated, no section restructuring. Sources referenced naturally per LinkedIn article style â€” no footnotes or citation numbers â€” by Basher
